    Opinions: \textit{\{BATTERY\#OPERATION\_PERFORMANCE, positive\}}, \\
    
    \textit{\{LAPTOP\#OPERATION\_PERFORMANCE, positive\}}\\



To be fed into the deep learning model, first the sentence needs to be tokenized, mapping words to their unique integer encoded counterparts. 
\paragraph{1 2 3 4 5 6 7 8 9 10 11 1 12 13 14 7 15 16 17}





\paragraph{the laptop was really good and it goes really fast just the way i thought it would of run"}





\section{Input parameters}
For the tuning of parameters of the input layer of the model, a simple deep learning model has been created with an embedding layer, max global pooling and a single 256 hidden layer with reLu activation before the sigmoid output layer. 

With all of these parameters keep constant, here is the results for different input hyper-parameters. 


\subsection{Word Vector Dimensionality}

As expected, the higher the dimensionality of the GloVe embedding, the better the model performed. However, it is interesting to note that the model performs best when it is allowed to adjust the embedding layer's weights, simply using the GloVe weights as a seed. This may be domain specific to the SemEval 2016 dataset, not having a high variation of different 








\subsection{Data Pre-Processing}
There are three main options for embedding words in text as vectors for use in unsupervised machine learning models. 
\subsection{Word Embeddings}
Representing sentences and principally words as vectors for use in data analysis remains a primary focus of the field of natural language processing. With the advent of  \textcolor{cite}{\cite{mikolov}}'s Word2Vec library, word embeddings were propelled to become the predominant approach for vectorizing textual data, as it significantly outperforms former methods of vectorization such as TF-IDF which indexes words on the basis of their frequencies in documents, and how important a word is relevant to a document in a collection. 
\newline\newline
The key assumption of all word embedding algorithms is that words with similar meanings appear in similar contexts, and attempt to capture this relation between words in their dense vector representation, with similar words having a similar representation. When word embeddings have been learnt, each word in the corpus will have an $n$-dimensional vector representation. 

\subsubsection{GloVe}

GloVe \textcolor{cite}{\cite{glove}} is a popular unsupervised learning algorithm, providing various dimensions of pre-trained word vectors from over 42Bn Wikipedia word tokens, which has proven itself to achieve 

\subsubsection{Embedding layer}
An embedding layer is an input layer to a neural network that has a weights that reflect learnt word embeddings. An embedding layer can learn embeddings for each word in the corpus as the model is trained, or alternatively it can take pre-trained word vectors such as GloVe as the weights, either choosing to use this as a seed, and updating the weights; or keeping the embedding static.

\newline

\paragraph{The laptop was really good and it goes really fast just the way I thought it would of run."}

The raw text first needs to be needs to be normalised, converting it all to lower or upper case, the words need to be split from punctuation leaving the sentence: 

\paragraph{the laptop was really good and it goes really fast just the way i thought it would of run"}

To be fed into the deep learning model, first the sentence needs to be tokenized, mapping words to their unique integer encoded counterparts. 
\paragraph{1 2 3 4 5 6 7 8 9 10 11 1 12 13 14 7 15 16 17}

\citep{te}





\subsection{Word Embeddings}
In NLP, the representation of words in a model still remains one of the predominant focuses of the field. \textcolor{cite}{\cite{mikolov}}'s word2vec library helped word embeddings become the predominant approach for vectorizing textual data, as it has proven that it significantly outperforms other methods of vectorization such as TF-IDF which indexes words on the basis of their frequencies in documents. Word embeddings work on the assumption that words with similar meanings appear in similar contexts, and capture this relation between words in their dense vector representation.
\newline
GloVe \textcolor{cite}{\cite{glove}} is a popular unsupervised learning algorithm packaged as a text file that provides vector representations of 400,000 words that were trained on a 2014 Wikipedia dump. The version referenced for this project contains embeddings in the dimensions: \{50, 100, 200, 300\}. \\
\begin{center}
    Text: \textit{Suffice it to say, my MacBook Pro keeps me going with its long battery life and blazing speed}.\\
\end{center}
The raw text above from the training dataset's sentence:292:2 first needs to be needs to be normalised, converting it all text to lower case. Next, the words need to be split from punctuation leaving the raw sentence text. Lastly, a stoplist is applied removing non-content words from the sentences. The example above will become:  

\begin{center}
    Text: \textit{suffice macbook pro long battery life blazing speed}
\end{center}




\subsubsection{Opinion Target Extraction}
% \textcolor{cite}{\citet{opinionwordexpansion}}
\textcolor{cite}{\citet{tohsu}} treated aspect target extraction as a sequence labelling problem, training sequence labeling classifiers with conditional random fields and using a bidirectional recurrent neural network. They achieved the best scoring system for Slot 2 for SemEval 2016 Task 5.  


In industry, even minimal increases in accuracy of a model could leads to millions of pounds of change in revenue, especially with inordinate amounts of customer reviews on sites like Amazon. 




\section{Progress}

Over the past month, I have predominantly been focusing on experimenting with TensorFlow and researching deep learning techniques, as this is where I wish to focus this project and where my passion for the field of sentiment analysis lies. 
I have excluded the data analysis section in this report, as I had not completed it to a standard I was pleased with, but I have begun experimentation with the SemEval 2016 Laptop dataset, having put it in a pandas data frame working towards an implementation of the SVM baseline systems. 

I had also completed extensive research on Adaptive Boosting as a machine learning technique, built from Random Forests which are built from decision trees, however taking Rob's advice I have decided to focus my attention on successfully implementing a deep learning technique. I have removed these sections as it seems to be counter-intuitive focusing on both AdaBoost in addition to deep neural networks; especially when AdaBoost will likely achieve only slightly better results than SVMs.

I have only tabulated the requirements of this project.
By the end of 2019; I intend to have a RNN implemented in TensorFlow for opinion target extraction, and will have completed the missing sections from this report, including data pre-processing techniques, further write-up of the SemEval dataset and finishing requirements and analysis.   

I will also find a way of better placing images in LaTex, so the layout of my final report is more refined. 
\newpage


Channels further rate of increases overfitting, as each channel acts as an accelerator for feature convergence as the model is fed the same input multiple times over each channel; which is concatenated, so the model can be understood to train on each input sample multiple times per epoch. 


The optimal hyperparameters will be selected by completing a random search. This is defined in Section \ref{bilstmtuning}.


The Bi-LSTM-CNN for subjectivity classification uses the hyperparameters discovered from the tuned Bi-LSTM-CNN model for aspect category detection, with a dropout of 0.3 in the penultimate layer and 3 channels with kernel sizes of [1,2,3] respectively.  \\
Both models achieve impressive results, with F1-Scores significantly above 0.9 for both. Figure \ref{fig:subjectivitycm} shows a confusion matrix for the models, and it can be seen that the SVM classifier with SGD is incredibly successful in identifying subjective statements correctly with over a 99\% accuracy. The baseline model actually achieves a higher accuracy than the Bi-LSTM CNN overall, however the F1-Score is lower. F1 is a better measure for performance than accuracy as the harmonic mean of recall and precision punishes misclassifications more; and is well suited for imbalanced data like SemEval's laptop reviews dataset. The Bi-LSTM achieves this higher F1-Score due to it classifying objective statements better, reducing the amount of objective statements classified as subjective.\\
Both models perform worse in attempting to classify objective statements, but this can be explained by the higher distribution of subjective statements in the training datasets, with only 21.6\% being subjective.  


For subjectivity classification however, it was found that monitoring val\_loss for early stopping did not work very well at all, with the training phase stopping at seemingly random places, making it hard to find an optimal value of $p$ with manual testing. For this reason, early stopping is not implemented with for subjectivity classification, and instead it runs for a fixed 75 epochs, which was found to give good results.
